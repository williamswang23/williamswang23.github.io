<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> deep networks for classification · Williams Wang's blog</title><meta name="description" content="deep networks for classification - Williams Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="https://i.imgur.com/8WpDtII.jpeg"><link rel="stylesheet" href="/hexo/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://williamswang23.github.io/atom.xml" title="Williams Wang's blog"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/hexo/atom.xml" title="Williams Wang's blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/hexo/" class="logo-link"><img src="https://i.imgur.com/8WpDtII.jpeg" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/hexo/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/hexo/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">deep networks for classification</h1><div class="post-info">Feb 16, 2021</div><div class="post-content"><h1 id="the-mathematic-perspective-of-DL"><a href="#the-mathematic-perspective-of-DL" class="headerlink" title="the mathematic perspective of DL"></a>the mathematic perspective of DL</h1><p><a target="_blank" rel="noopener" href="https://book-wright-ma.github.io/">High-Dimensional Data Analysis by John Wright and Yi Ma</a></p>
<hr>
<p>the 16 chapter: deep networks for classification</p>
<a id="more"></a>
<hr>
<h2 id="review"><a href="#review" class="headerlink" title="review"></a>review</h2><ol>
<li>the image classification is most representative case in DL field,because of explainable highly in math principle. </li>
<li>DL for classification and supervised learning in this chapter.</li>
<li>the black box of DL could be consider as a mapping problem: $f(.):x \mapsto y$ and the function as: $f: R^n \rightarrow R^k$.</li>
<li>each layer could be represent as: $x_{l+1}=f^l(z_{l})=\phi(W_{l}z_{l})$. the total mapping of DL could be represent as: $f(x, \theta)=\phi(W_{L-1}\phi(…\phi(W_{1\phi(W_{0}x)})))$, $\theta$ is parameters of parameters. the $\phi$ function are recursively from input to L-1 layer. the training process target is finding optimal parameters with minimize loss function.</li>
<li>minimizing loss function: cross-entropy loss, $min_{\theta \in \Theta} L_{CE}(\theta, x, y)=-E[&lt;y, log[f(x,\theta)&gt;]$, the optimization process based on SGD.</li>
<li>isometry is an considerable feature in analytical process.</li>
<li>from a full or complex matrix to sparse matrix, dimension deduction is a typical tool. finding a way to transform origin matrix to sparse or low rank ones.</li>
<li>dropout is also a important tool to deduct dimension of matrix.</li>
<li>$non-linear data \mapsto non-linear sub-manifold \mapsto discriminative output$</li>
<li>it should noticeable that only in one assumption, which the distribution of each class has relatively low dimensional intrinsic structure, the DL network could solve problem efficiently. in addition, the distribution of each class has a support on a low dimensional submanifold.<br><img src="https://i.imgur.com/VgGzKjG.jpg" alt="submanifold"></li>
<li><blockquote>
<p>we want to learn a smooth mapping $z=f(x,\theta)$ that maps each of thesubmanifolds $M_{j} \subset R^n$ to a linear subspace $S_j \subset R^d$.</p>
</blockquote>
</li>
<li><blockquote>
<p>The more ideal case when the data X lie on linear subspaces has been systematically studied as generalized principal component analysis (GPCA)</p>
</blockquote>
</li>
<li><blockquote>
<p>rate distortion: Given a random variable z and a prescribed precision ε &gt; 0, the rate distortion R(z, ε) is the minimal number of binary bits needed to encode z such that the expected decoding error 16 is less than ε.</p>
</blockquote>
</li>
<li><blockquote>
<p>$R(X,\epsilon)=\frac{1}{2}log det(I+\frac{d}{m\epsilon^2}ZZ^*)$,log det(·) function is a smooth but nonconvex surrogate for promoting low-dimensionality of the representation Z.</p>
</blockquote>
</li>
<li><blockquote>
<p>As it tightly characterizes the rate distortion of Gaussian or subspace-like distributions, finding the clustering Π that minimizes: $min_{\Pi} R^c(Z,\epsilon|\Pi)$.</p>
</blockquote>
</li>
<li>information theory provide an efficient tool to compress matrix or finding the low dimensional space, with rate distorting indicator.</li>
<li> $\Delta R(Z,\Pi, \epsilon)= R(Z,\epsilon)-R^c(X,\epsilon|\Pi)$, Therefore, a good representation Z of X is one such that, given a partition Π of Z, achieves a large diﬀerence between the coding rate for the whole and the average rate for all the subsets:</li>
<li><blockquote>
<p>∆R is monotonic in the scale of the features Z.</p>
</blockquote>
</li>
<li><blockquote>
<p>Once the representations are comparable, our goal becomes to learn a set of features Z(θ) = f(X, θ) and their partition Π (if not given in advance) such that they maximize the reduction between the coding rate of all features and that of the sum of features w.r.t. their classes:$ max_{\theta, \Pi}=\Delta R(Z(\Theta,\Pi,\epsilon)).</p>
</blockquote>
</li>
<li><blockquote>
<p>The maximal coding rate reduction can be viewed as a generalization to Information Gain (IG), which aims to maximize the reduction of entropy of a random variable, say z, with . respect to an observed attribute, say π: max π IG(z, π) = H(z) − H(z | π), i.e., the mutual information between z and π</p>
</blockquote>
</li>
<li><strong>maximal coding rate reduction ($MCR^2$)</strong></li>
<li><img src="https://i.imgur.com/pyi7TDy.jpg" alt="reducedrate"></li>
<li><img src="https://i.imgur.com/thd4F50.jpg" alt="theorem162"></li>
<li><blockquote>
<p>By applying exact the same training parameters, MCR 2 is signiﬁcantly more robust than CE, especially with higher ratio of corrupted labels.</p>
</blockquote>
</li>
<li>chapter 16.3 is too complex to representation,you can read it by yourself from above download link.</li>
<li><blockquote>
<p>As this deep network is derived from maximizing the rate reduced, we call it the ReduNet.</p>
</blockquote>
</li>
<li><img src="https://i.imgur.com/6vhze3U.jpg" alt="proposition165"></li>
<li><img src="https://i.imgur.com/stFhRIu.jpg]" alt="overviewofmulticlasssginals"></li>
</ol>
<h2 id="conclusion"><a href="#conclusion" class="headerlink" title="conclusion"></a>conclusion</h2><p>this paper has two stunning aspects for me. first of all, it gives me an alternative prospective to interpret DL and NN, and make two points about dimension deduction technology, one is mapping, other is rate distorting. both of them make an attempt to find another more efficient way to solve DL&amp;discrimination problem. secondly, the discussion is not complete, and lacking enough information and evidences about DL&amp;NN principle, especially details about dynamic process of DL NN.</p>
</div></article></div></main><footer><div class="paginator"><a href="/hexo/2021/02/19/information-and-DNN/" class="prev">PREV</a><a href="/hexo/2021/02/15/%E7%AE%A1%E7%90%86/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2023 <a href="http://williamswang23.github.io">Williams Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>