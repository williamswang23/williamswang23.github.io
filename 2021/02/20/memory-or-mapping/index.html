<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> memory or mapping · Williams Wang's blog</title><meta name="description" content="memory or mapping - Williams Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="https://i.imgur.com/8WpDtII.jpeg"><link rel="stylesheet" href="/hexo/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://williamswang23.github.io/atom.xml" title="Williams Wang's blog"><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/hexo/atom.xml" title="Williams Wang's blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/hexo/" class="logo-link"><img src="https://i.imgur.com/8WpDtII.jpeg" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/hexo/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/hexo/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">memory or mapping</h1><div class="post-info">Feb 20, 2021</div><div class="post-content"><h1 id="memory-or-mapping"><a href="#memory-or-mapping" class="headerlink" title="memory or mapping"></a>memory or mapping</h1><p>“Understanding deep learning requires rethinking generalization”</p>
<blockquote>
<p>to be or not to be? – William Shakespeare</p>
</blockquote>
<p><strong>when a NN was processing, is it mapping or memorizing?</strong></p>
<hr>
<a id="more"></a>
<hr>
<h2 id="review"><a href="#review" class="headerlink" title="review"></a>review</h2><ol>
<li><blockquote>
<p>Moreover, when the number of parameters is large, theory suggests that some form of regularization is needed to ensure small generalization error</p>
</blockquote>
</li>
<li><blockquote>
<p><strong>Deep neural networks easily ﬁt random labels.</strong></p>
</blockquote>
</li>
<li><blockquote>
<p>While simple to state, this observation has profound implications from a statistical learning perspective:</p>
</blockquote>
<ol>
<li>The effective capacity of neural networks is sufﬁcient for memorizing the entire data set.</li>
<li>Even optimization on random labels remains easy. In fact, training time increases only by a small constant factor compared with training on the true labels.</li>
<li>Randomizing labels is solely a data transformation, leaving all other properties of the learning problem unchanged.</li>
</ol>
</li>
<li><blockquote>
<p>This shows that despite their structure, convolutional neural nets can ﬁt random noise.</p>
</blockquote>
</li>
<li><blockquote>
<p>We observe a steady deterioration of the generalization error as we increase the noise level. This shows that neural networks are able to capture the remaining signal in the data, while at the same time ﬁt the noisy part using brute-force.</p>
</blockquote>
</li>
<li><blockquote>
<p>Explicit regularization may improve generalization performance, but is neither necessary nor by itself sufﬁcient for controlling generalization error.</p>
</blockquote>
</li>
<li><blockquote>
<p>Our goal is to understand the effective model capacity of feed-forward neural networks. Toward this goal, we choose a methodology inspired by non-parametric randomization tests. Speciﬁcally, we take a candidate architecture and train it both on the true data and on a copy of the data in which the true labels were replaced by random labels.</p>
</blockquote>
</li>
<li><blockquote>
<p>To our surprise, several properties of the training process for multiple standard achitectures is largely unaffected by this transformation of the labels.</p>
</blockquote>
</li>
<li><blockquote>
<p>Surprisingly, stochastic gradient descent with unchanged hyperparameter settings can optimize the weights to ﬁt to random labels perfectly, even though the random labels completely destroy the relationship between images and labels.</p>
</blockquote>
</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rademacher_complexity">Rademacher complexity - Wikipedia</a></li>
<li><blockquote>
<p>It conﬁrms that early stopping could potentially improve the generalization performance.</p>
</blockquote>
</li>
<li><blockquote>
<p>Batch normalization (Ioffe &amp; Szegedy, 2015) is an operator that normalizes the layer responses within each mini-batch.</p>
</blockquote>
</li>
<li><blockquote>
<p><strong>Theorem 1. There exists a two-layer neural network with ReLU activations and 2n+d weights that can represent any function on a sample of size n in d dimensions.</strong></p>
</blockquote>
</li>
</ol>
<h2 id="discussion"><a href="#discussion" class="headerlink" title="discussion"></a>discussion</h2><p>this paper is too simple to comprehend authors’ argument and evidences. but it inspires me to think how to understanding a NN model’s rule. the NN model just memorize training set or has inferring capacity with any ingenious architectures?</p>
</div></article></div></main><footer><div class="paginator"><a href="/hexo/2021/02/20/%E9%98%85%E8%AF%BB%E4%B8%8E%E5%86%99%E4%BD%9C/" class="prev">PREV</a><a href="/hexo/2021/02/19/DL-NN-geometrical-perspective/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2023 <a href="http://williamswang23.github.io">Williams Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>